{"cells":[{"cell_type":"markdown","metadata":{"id":"u7Ec_bJB0z4c"},"source":["# Chapter11 系列変換"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9euDvmga1DWJ"},"outputs":[],"source":["%tensorflow_version 2.x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QFvD69k45BqF"},"outputs":[],"source":["!pip install janome nltk tensorflow==2.4.0"]},{"cell_type":"markdown","metadata":{"id":"yBLmjtJ8047i"},"source":["## 系列変換モデルの実装"]},{"cell_type":"markdown","metadata":{"id":"6Zcba6bM08J8"},"source":["### データセットの準備"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ar2Vz5fH0wYG"},"outputs":[],"source":["!mkdir data\n","!mkdir models\n","!wget http://www.manythings.org/anki/jpn-eng.zip -P data/\n","!unzip data/jpn-eng.zip -d data/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vxddgmzu1Fol"},"outputs":[],"source":["from collections import defaultdict\n","\n","import numpy as np\n","import tensorflow as tf\n","from janome.tokenizer import Tokenizer\n","from nltk.translate.bleu_score import corpus_bleu\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.layers import Dense, Input, Embedding, GRU, Dot, Activation, Concatenate\n","from tensorflow.keras.models import Model, model_from_json\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ai0lyxfV2Lf8"},"outputs":[],"source":["batch_size = 32\n","epochs = 20\n","model_path = 'models/mode.h5'\n","enc_arch = 'models/encoder.json'\n","dec_arch = 'models/decoder.json'\n","data_path = 'data/jpn.txt'\n","num_words = 10000\n","num_data = 20000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tkCzISUX2Nlg"},"outputs":[],"source":["def load_dataset(filename):\n","    en_texts = []\n","    ja_texts = []\n","    with open(filename) as f:\n","        for line in f:\n","            en_text, ja_text = line.strip().split('\\t')[:2] #3個目以降のデータは不要\n","            en_texts.append(en_text)\n","            ja_texts.append(ja_text)\n","    return en_texts, ja_texts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3mAq7ct5RBQ"},"outputs":[],"source":["en_texts, ja_texts = load_dataset(data_path)\n","en_texts, ja_texts = en_texts[:num_data], ja_texts[:num_data]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_9UPXIXX5TAt"},"outputs":[],"source":["t = Tokenizer(wakati=True)\n","\n","#分かち書き\n","def tokenize(text):\n","    return t.tokenize(text)\n","\n","#ボキャブラリの作成\n","def build_vocabulary(texts, num_words=None):\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","        num_words=num_words, oov_token='<UNK>', filters=''\n","    )\n","    tokenizer.fit_on_texts(texts)\n","    return tokenizer\n","\n","#<start> <end>記号の付与\n","def preprocess_dataset(texts):\n","    return ['<start> {} <end>'.format(text) for text in texts]\n","\n","#品詞ごとにスペース区切り\n","def preprocess_ja(texts):\n","    return [' '.join(tokenize(text)) for text in texts]\n","\n","def create_dataset(en_texts, ja_texts, en_vocab, ja_vocab):\n","    en_seqs = en_vocab.texts_to_sequences(en_texts)\n","    ja_seqs = ja_vocab.texts_to_sequences(ja_texts)\n","    en_seqs = pad_sequences(en_seqs, padding='post')\n","    ja_seqs = pad_sequences(ja_seqs, padding='post')\n","    return [en_seqs, ja_seqs[:, :-1]], ja_seqs[:, 1:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Slgtw9Yv95Mw"},"outputs":[],"source":["ja_texts = preprocess_ja(ja_texts)\n","ja_texts = preprocess_dataset(ja_texts)\n","en_texts = preprocess_dataset(en_texts)\n","x_train, x_test, y_train, y_test = train_test_split(en_texts, ja_texts, test_size=0.2, random_state=42)\n","en_vocab = build_vocabulary(x_train, num_words)\n","ja_vocab = build_vocabulary(y_train, num_words)\n","x_train, y_train = create_dataset(x_train, y_train, en_vocab, ja_vocab)"]},{"cell_type":"markdown","metadata":{"id":"IAS93hbV-7XB"},"source":["### モデルの定義"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ne7PtUhk-C8m"},"outputs":[],"source":["# エンコーダとデコーダで共通するメソッド\n","class BaseModel:\n","\n","    def build(self):\n","        raise NotImplementedError()\n","\n","    #モデルアーキテクチャの保存\n","    def save_as_json(self, filepath):\n","        model = self.build()\n","        with open(filepath, 'w') as f:\n","            f.write(model.to_json())\n","    #モデルの読み込み\n","    @classmethod\n","    def load(cls, architecture_file, weight_file, by_name=True):\n","        with open(architecture_file) as f:\n","            model = model_from_json(f.read())\n","            model.load_weights(weight_file, by_name=by_name)\n","            return model\n","\n","class Encoder(BaseModel):\n","\n","    def __init__(self, input_dim, emb_dim=300, hid_dim=256, return_sequences=False):\n","        self.input = Input(shape=(None,), name='encoder_input')\n","        self.embedding = Embedding(input_dim=input_dim,\n","                                   output_dim=emb_dim,\n","                                   mask_zero=True,\n","                                   name='encoder_embedding') #one-hotベクトルを分散表現に変換\n","        self.gru = GRU(hid_dim,\n","                       return_sequences=return_sequences,\n","                       return_state=True,\n","                       name='encoder_gru')#RNN層\n","\n","    def __call__(self):\n","        x = self.input\n","        embedding = self.embedding(x)#one-hotベクトルを分散表現に変換\n","        output, state = self.gru(embedding)#RNN層に入力\n","        return output, state \n","\n","    def build(self):\n","        output, state = self()\n","        return Model(inputs=self.input, outputs=[output, state])\n","\n","class Decoder(BaseModel):\n","\n","    def __init__(self, output_dim, emb_dim=300, hid_dim=256):\n","        self.input = Input(shape=(None,), name='decoder_input')\n","        self.embedding = Embedding(input_dim=output_dim,\n","                                   output_dim=emb_dim,\n","                                   mask_zero=True,\n","                                   name='decoder_embedding')#one-hotベクトルを分散表現に変換\n","        self.gru = GRU(hid_dim,\n","                       return_sequences=True,\n","                       return_state=True,\n","                       name='decoder_gru')\n","        self.dense = Dense(output_dim, activation='softmax', name='decoder_output')#出力層\n","\n","        # for inference.\n","        self.state_input = Input(shape=(hid_dim,), name='decoder_state_in')\n","\n","    def __call__(self, states, enc_output=None):\n","        x = self.input\n","        embedding = self.embedding(x)\n","        outputs, state = self.gru(embedding, initial_state=states)\n","        outputs = self.dense(outputs) \n","        return outputs, state \n","\n","    def build(self):\n","        decoder_output, decoder_state = self(states=self.state_input)\n","        return Model(\n","            inputs=[self.input, self.state_input],\n","            outputs=[decoder_output, decoder_state])\n","\n","class Seq2seq(BaseModel):\n","\n","    def __init__(self, encoder, decoder):\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def build(self):\n","        encoder_output, state = self.encoder() \n","        decoder_output, _ = self.decoder(states=state, enc_output=encoder_output)\n","        return Model([self.encoder.input, self.decoder.input], decoder_output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KbrahqO5KbF_"},"outputs":[],"source":["encoder = Encoder(num_words)\n","decoder = Decoder(num_words)\n","seq2seq = Seq2seq(encoder, decoder)\n","model = seq2seq.build()\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"CMjUOJUnKb4N","outputId":"cab50144-71a3-42ab-a087-640e206c8b84"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","450/450 [==============================] - 186s 413ms/step - loss: 1.6080 - val_loss: 1.2773\n","Epoch 2/20\n","450/450 [==============================] - 183s 407ms/step - loss: 1.1090 - val_loss: 1.0744\n","Epoch 3/20\n","450/450 [==============================] - 180s 400ms/step - loss: 0.9101 - val_loss: 0.9800\n","Epoch 4/20\n","450/450 [==============================] - 180s 400ms/step - loss: 0.7634 - val_loss: 0.9274\n","Epoch 5/20\n","450/450 [==============================] - 180s 400ms/step - loss: 0.6413 - val_loss: 0.8830\n","Epoch 6/20\n","450/450 [==============================] - 180s 400ms/step - loss: 0.5356 - val_loss: 0.8609\n","Epoch 7/20\n","450/450 [==============================] - 179s 398ms/step - loss: 0.4453 - val_loss: 0.8502\n","Epoch 8/20\n","450/450 [==============================] - 180s 400ms/step - loss: 0.3679 - val_loss: 0.8452\n","Epoch 9/20\n","450/450 [==============================] - 180s 400ms/step - loss: 0.3035 - val_loss: 0.8470\n","Epoch 10/20\n","450/450 [==============================] - 180s 399ms/step - loss: 0.2505 - val_loss: 0.8513\n","Epoch 11/20\n","450/450 [==============================] - 180s 399ms/step - loss: 0.2064 - val_loss: 0.8649\n"]}],"source":["callbacks = [\n","    EarlyStopping(patience=3),\n","    ModelCheckpoint(model_path, save_best_only=True, save_weights_only=True)\n","]\n","model.fit(x=x_train,\n","          y=y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          callbacks=callbacks,\n","          validation_split=0.1)\n","\n","encoder.save_as_json(enc_arch)\n","decoder.save_as_json(dec_arch)"]},{"cell_type":"markdown","metadata":{"id":"g1h9h2G6EHhx"},"source":["### 予測用クラスの実装"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IfjBMDSL-DcL"},"outputs":[],"source":["class InferenceAPI:\n","\n","    def __init__(self, encoder_model, decoder_model, en_vocab, ja_vocab):\n","        self.encoder_model = encoder_model\n","        self.decoder_model = decoder_model\n","        self.en_vocab = en_vocab\n","        self.ja_vocab = ja_vocab\n","\n","    def predict(self, text):\n","        output, state = self._compute_encoder_output(text)\n","        sequence = self._generate_sequence(output, state)\n","        decoded = self._decode(sequence)\n","        return decoded\n","  \n","    #入力文を固定長に変換\n","    def _compute_encoder_output(self, text):\n","        x = self.en_vocab.texts_to_sequences([text])\n","        output, state = self.encoder_model.predict(x)\n","        return output, state\n","\n","    def _compute_decoder_output(self, target_seq, state, enc_output=None):\n","        output, state = self.decoder_model.predict([target_seq, state])\n","        return output, state\n","        \n","    #日本語の単語に対応するIDを生成\n","    def _generate_sequence(self, enc_output, state, max_seq_len=50):\n","        target_seq = np.array([self.ja_vocab.word_index['<start>']])\n","        sequence = []\n","        for i in range(max_seq_len):\n","            output, state = self._compute_decoder_output(target_seq, state, enc_output)\n","            sampled_token_index = np.argmax(output[0, 0])\n","            if sampled_token_index == self.ja_vocab.word_index['<end>']:\n","                break\n","            sequence.append(sampled_token_index)\n","            target_seq = np.array([sampled_token_index])\n","        return sequence\n","\n","    #IDを文字列に変換\n","    def _decode(self, sequence):\n","        decoded = self.ja_vocab.sequences_to_texts([sequence])\n","        decoded = decoded[0].split(' ')\n","        return decoded"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"loZ8yO5wIbb0"},"outputs":[],"source":["def evaluate_bleu(X, y, api):\n","    d = defaultdict(list)\n","    for source, target in zip(X, y):\n","        d[source].append(target)\n","    hypothesis = []\n","    references = []\n","    for source, targets in d.items():\n","        pred = api.predict(source)\n","        hypothesis.append(pred)\n","        references.append(targets)\n","    bleu_score = corpus_bleu(references, hypothesis)\n","    return bleu_score"]}],"metadata":{"colab":{"name":"Chapter11.ipynb","provenance":[],"authorship_tag":"ABX9TyNbhh4O+skue3KDsp7tlbHZ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}